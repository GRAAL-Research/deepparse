<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Training Guide &mdash; deepparse 0.9.13 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Retrain an Address Parser for Single Country Uses" href="examples/single_country_retrain.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html">
            
              <img src="_static/deepparse.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                0.9.13
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="install/installation.html">Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="get_started/get_started.html">Getting Started</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="parser.html">Parser</a></li>
<li class="toctree-l1"><a class="reference internal" href="pre_processor.html">Pre-Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset_container.html">Dataset Container</a></li>
<li class="toctree-l1"><a class="reference internal" href="comparer.html">Comparer</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">CLI</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">Parse Address With Our Out-Of-The-Box API</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="examples/parse_addresses.html">Parse Addresses</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples/parse_addresses_uri.html">Parse Addresses Using A URI</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples/parse_addresses_with_cli.html">Parse Addresses Using Our CLI</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples/retrained_model_parsing.html">Use a Retrained Model to Parse Addresses</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples/fine_tuning.html">Retrain a Pretrained Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples/fine_tuning_uri.html">Retrain a Pretrained Model Using A URI</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples/fine_tuning_with_csv_dataset.html">Retrain a Pretrained Model Using a CSV Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples/retrain_attention_model.html">Retrain an Attention Mechanism Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples/retrain_with_new_prediction_tags.html">Retrain With New Prediction Tags</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples/retrain_with_new_seq2seq_params.html">Retrain With New Seq2Seq Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples/single_country_retrain.html">Retrain an Address Parser for Single Country Uses</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model training</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Training Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#learning-successfully">Learning Successfully</a></li>
<li class="toctree-l2"><a class="reference internal" href="#do-not-forget">Do Not Forget!</a></li>
<li class="toctree-l2"><a class="reference internal" href="#about-the-data">About The Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="#modifying-the-architecture">Modifying the Architecture</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">deepparse</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Training Guide</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/training_guide.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="training-guide">
<h1>Training Guide<a class="headerlink" href="#training-guide" title="Permalink to this heading"></a></h1>
<p>In addition to parsing addresses out-of-the-box, Deepparse allows you to retrain the pre-trained models to fit your data and use cases.
In the world of machine learning, this is what’s referred to as <code class="docutils literal notranslate"><span class="pre">fine-tuning</span></code>, which can make it easier to obtain well-performing
models more efficiently and with less data.</p>
<p>Since fine-tuning models can be tricky, this section of the documentation provides some guidelines and insights that may
be useful to adapt our models successfully. See <a class="reference internal" href="examples/fine_tuning.html#fine-tuning"><span class="std std-ref">Retrain a Pretrained Model</span></a> for a coding example of
how to retrain our models.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We provide practical recommendations for fine-tuning, but you may have to try multiple retraining configurations to
achieve an optimal result. If you have difficulty adapting our models to your use case,
open an issue on the Deepparse <a class="reference external" href="https://github.com/GRAAL-Research/deepparse/issues">GitHub</a> page.</p>
</div>
<p>A few use cases may lead you to want to retrain Deepparse’s models. Whether you wish to obtain a better
performance on a single or multiple countries that our models weren’t trained on, or your data and address schemes require a more complex
architecture, or your dataset’s tag structure, differs from ours; Deepparse’s retraining features accommodate all these use cases and more.</p>
<p>In practice, our models were trained on 20 countries. They demonstrated very accurate results on all of them, so we advise you to use our models without retraining unless you wish to predict
different tags (e.g., StreetNumber, …). Also, suppose you want to retrain
our models to perform better on countries outside of the 20 used in the original training set. In that case, you can look
at <a class="reference external" href="https://github.com/GRAAL-Research/deepparse-address-data">our dataset</a> which includes an additional 41 countries used only for testing.</p>
<p>There are two main concerns to keep in mind when fine-tuning a model: the model’s convergence (i.e., its ability actually to learn from the new data)
and the possibility of <code class="docutils literal notranslate"><span class="pre">catastrophic</span> <span class="pre">forgetting</span></code> (i.e., losing the model’s previous knowledge after training on the new data).</p>
<section id="learning-successfully">
<h2>Learning Successfully<a class="headerlink" href="#learning-successfully" title="Permalink to this heading"></a></h2>
<p>Making a model converge is as much an art as a science since it often requires a lot of experimentation and parameter tuning. In the case
of fine-tuning, the models have already developed a base knowledge of the task that they were trained on, which gives them an edge.
This is especially true in the case of Deepparse since the task you are fine-tuning remains the same (i.e. parsing addresses).
However, there are a couple of points to consider to obtain favourable results:</p>
<ul class="simple">
<li><p><strong>Make sure you have enough data</strong>: deep learning models are notorious for being pretty data-hungry, so unless you have enough data, the models
will have a hard time learning. Since Deepparse’s models have already been trained on a few million addresses, the need for data is mitigated for fine-tuning. However,
it is recommended to use at least a few thousand examples per new country when retraining.</p></li>
<li><p><strong>Prepare your dataset</strong>: once you are done pre-processing your dataset, you must convert it to a format which can be loaded into
a <a class="reference internal" href="dataset_container.html#deepparse.dataset_container.DatasetContainer" title="deepparse.dataset_container.DatasetContainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">DatasetContainer</span></code></a>. See the <a class="reference internal" href="dataset_container.html#dataset-container"><span class="std std-ref">Dataset Container</span></a> section for more details.
Also, make sure to keep a portion of your data apart to test the performance of your retrained models.</p></li>
<li><p><strong>Use a proper learning rate</strong>: if you are unfamiliar with gradient descent and neural network optimization, you probably don’t know what
a <code class="docutils literal notranslate"><span class="pre">learning</span> <span class="pre">rate</span></code> is. But have no fear; you do not need a Ph.D. to retrain deepparse’s models. All you need to understand is that a learning rate
is a value that guides the training process. When it comes to fine-tuning, it is recommended to use a learning rate lower than the one used for the first
training, in this case, we recommend using a learning rate lower than <code class="docutils literal notranslate"><span class="pre">0.1</span></code>. This parameter can be changed in the <a class="reference internal" href="parser.html#deepparse.parser.AddressParser.retrain" title="deepparse.parser.AddressParser.retrain"><code class="xref py py-meth docutils literal notranslate"><span class="pre">retrain()</span></code></a> method.</p></li>
<li><p><strong>Train for long enough</strong>: Deepparse’s models are based on the LSTM neural network architecture, which may require a few more training epochs
than recent architectures for fine-tuning. The number of epochs would depend on the use case, but allowing the models to train long enough is important. Perhaps start somewhere between 5 and 10 epochs and increase the number of epochs if needed.</p></li>
<li><p><strong>Use a GPU</strong>: this is not required for retraining, but it is highly recommended to use a GPU if your device has one to speed up the
training process. This can be specified in the <a class="reference internal" href="parser.html#deepparse.parser.AddressParser" title="deepparse.parser.AddressParser"><code class="xref py py-class docutils literal notranslate"><span class="pre">AddressParser</span></code></a> constructor.</p></li>
</ul>
</section>
<section id="do-not-forget">
<h2>Do Not Forget!<a class="headerlink" href="#do-not-forget" title="Permalink to this heading"></a></h2>
<p>As mentioned above, catastrophic forgetting can happen when fine-tuning machine learning models. This is because the models’ internal parameters are
modified to accommodate the new task/data, which can impact their ability to be appropriate for the previous task/data.</p>
<p>There are many fancy ways to mitigate catastrophic forgetting when fine-tuning models. Still, given the task and data that Deepparse handles, we recommend including some of the previous data when constructing your retraining dataset. The amount
of addresses to keep would vary depending on the number of new addresses, but somewhere between 1% and 10% would be a good start.</p>
<p>Another approach that can help reduce the effect of forgetting is freezing part of the model. Check out
the <a class="reference internal" href="parser.html#deepparse.parser.AddressParser.retrain" title="deepparse.parser.AddressParser.retrain"><code class="xref py py-meth docutils literal notranslate"><span class="pre">retrain()</span></code></a> method for more details on how to freeze different parts of our models during retraining.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you’re only interested in the models’ performance on the new data, you should not concern yourself with catastrophic forgetting.</p>
</div>
</section>
<section id="about-the-data">
<h2>About The Data<a class="headerlink" href="#about-the-data" title="Permalink to this heading"></a></h2>
<p>Deepparse’s models learn in a supervised manner; this means that the data provided for retraining must be labelled (i.e. the tag of each element in the
address needs to be specified). This is also required when you want to retrain our models with your own custom tags. Each word in the address must
have a corresponding tag. If you are using custom tags, they must be defined in the <a class="reference internal" href="parser.html#deepparse.parser.AddressParser.retrain" title="deepparse.parser.AddressParser.retrain"><code class="xref py py-meth docutils literal notranslate"><span class="pre">retrain()</span></code></a> method under
the <code class="docutils literal notranslate"><span class="pre">prediction_tags</span></code> argument. Here are some examples of properly labelled addresses:</p>
<img alt="_images/labeled_addresses.png" src="_images/labeled_addresses.png" />
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the main objective of retraining is to introduce different tags, it might be a good idea to freeze the model layers. This will speed up the
retraining process and will probably yield good results, especially if you are training on the same countries as the original training set.</p>
</div>
<p>In case your data is mostly or exclusively unlabeled, you can retrain on the labelled portion and then use the obtained model to predict labels
for a few more randomly chosen unlabeled addresses, verify and correct the predictions and retrain with the newly labelled addresses added to the retraining dataset.
This will allow you to incrementally increase the size of your dataset with the help of the models. This is a simple case of <em>active learning</em>.</p>
</section>
<section id="modifying-the-architecture">
<h2>Modifying the Architecture<a class="headerlink" href="#modifying-the-architecture" title="Permalink to this heading"></a></h2>
<p>The <a class="reference internal" href="parser.html#deepparse.parser.AddressParser.retrain" title="deepparse.parser.AddressParser.retrain"><code class="xref py py-meth docutils literal notranslate"><span class="pre">retrain()</span></code></a> method allows you to change the architecture of the models using the <code class="docutils literal notranslate"><span class="pre">seq2seq_params</span></code>
argument. This can be useful if you need a more complex model or a lighter model, for example. However, if you
change the models’ architecture, a completely new model will be retrained from scratch. This
means that all the previous knowledge that the initial model had will disappear.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="examples/single_country_retrain.html" class="btn btn-neutral float-left" title="Retrain an Address Parser for Single Country Uses" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020-2024, Marouane Yassine &amp; David Beauchemin.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>