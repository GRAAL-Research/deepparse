#!/usr/bin/env python
# coding=utf-8
import os
import warnings

import torch

os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

import logging
import os
from dataclasses import dataclass, field
from typing import Optional

import numpy as np
import pycountry as pycountry
from datasets import load_dataset, load_metric
from transformers import (
    AutoConfig,
    AutoModelForTokenClassification,
    AutoTokenizer,
    DataCollatorForTokenClassification,
    HfArgumentParser,
    Trainer,
    TrainingArguments,
    set_seed,
)

logger = logging.getLogger(__name__)


@dataclass
class ModelArguments:
    """
    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
    """

    model_name_or_path: str = field(
        metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
    )
    config_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
    )
    tokenizer_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
    )
    cache_dir: Optional[str] = field(
        default=None,
        metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
    )
    model_revision: str = field(
        default="main",
        metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
    )
    use_auth_token: bool = field(
        default=False,
        metadata={
            "help": "Will use the token generated when running `transformers-cli login` (necessary to use this script "
                    "with private models)."
        },
    )


@dataclass
class DataTrainingArguments:
    """
    Arguments pertaining to what data we are going to input our model for training and eval.
    """

    task_name: Optional[str] = field(default="ner", metadata={"help": "The name of the task (ner, pos...)."})
    dataset_name: Optional[str] = field(
        default=None, metadata={"help": "The name of the dataset to use (via the datasets library)."}
    )
    dataset_config_name: Optional[str] = field(
        default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
    )
    train_dir: Optional[str] = field(
        default=None, metadata={"help": "The input training data dir."}
    )
    test_dir: Optional[str] = field(
        default=None, metadata={"help": "The input test data dir."}
    )
    overwrite_cache: bool = field(
        default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
    )
    label_all_tokens: bool = field(
        default=False,
        metadata={
            "help": "Whether to put the label for one word on all tokens of generated by that word or just on the "
                    "one (in which case the other tokens will have a padding index)."
        },
    )
    return_entity_level_metrics: bool = field(
        default=False,
        metadata={"help": "Whether to return all the entity levels during evaluation or just the overall ones."},
    )


def main():
    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()

    # training_args.num_train_epochs = 1
    # training_args.per_device_train_batch_size = 64
    # training_args.per_device_eval_batch_size = 128

    text_column_name = "tokens"
    label_column_name = "tags"

    label_list = [
        "StreetNumber", "Unit", "StreetName", "Orientation", "Municipality", "Province", "PostalCode",
        "GeneralDelivery"
    ]
    label_to_id = {l: i for i, l in enumerate(label_list)}

    # set_seed(training_args.seed)

    # Load pretrained model and tokenizer
    config = AutoConfig.from_pretrained(
        model_args.config_name if model_args.config_name else model_args.model_name_or_path,
        num_labels=len(label_list),
        finetuning_task=data_args.task_name,
        cache_dir=model_args.cache_dir,
        revision=model_args.model_revision,
        use_auth_token=True if model_args.use_auth_token else None,
    )
    tokenizer = AutoTokenizer.from_pretrained(
        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
        cache_dir=model_args.cache_dir,
        use_fast=True,
        revision=model_args.model_revision,
        use_auth_token=True if model_args.use_auth_token else None,
    )
    model = AutoModelForTokenClassification.from_pretrained(
        model_args.model_name_or_path,
        from_tf=bool(".ckpt" in model_args.model_name_or_path),
        config=config,
        cache_dir=model_args.cache_dir,
        revision=model_args.model_revision,
        use_auth_token=True if model_args.use_auth_token else None,
    )
    # device = "cuda:0" if torch.cuda.is_available() else "cpu"
    # model.to(device)

    def tokenize_and_align_labels(examples):
        tokenized_inputs = tokenizer(
            examples[text_column_name],
            padding=False,
            truncation=True,
            is_split_into_words=True
        )
        labels = []
        for i, label in enumerate(examples[label_column_name]):
            word_ids = tokenized_inputs.word_ids(batch_index=i)
            previous_word_idx = None
            label_ids = []
            for word_idx in word_ids:
                # Special tokens have a word id that is None. We set the label to -100 so they are automatically
                # ignored in the loss function.
                if word_idx is None:
                    label_ids.append(-100)
                # We set the label for the first token of each word.
                elif word_idx != previous_word_idx:
                    label_ids.append(label_to_id[label[word_idx]])
                # For the other tokens in a word, we set the label to either the current label or -100, depending on
                # the label_all_tokens flag.
                else:
                    label_ids.append(label_to_id[label[word_idx]] if data_args.label_all_tokens else -100)
                previous_word_idx = word_idx

            labels.append(label_ids)
        tokenized_inputs["labels"] = labels
        return tokenized_inputs

    # Metrics
    metric = load_metric("seqeval", zero_division=0)

    def compute_metrics(p):
        predictions, labels = p
        predictions = np.argmax(predictions, axis=2)

        # Remove ignored index (special tokens)
        true_predictions = [
            [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
            for prediction, label in zip(predictions, labels)
        ]
        true_labels = [
            [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
            for prediction, label in zip(predictions, labels)
        ]
        with warnings.catch_warnings():  # We skip UserWarnings since our NE tags are not typical NE tags.
            warnings.simplefilter("ignore")
            results = metric.compute(predictions=true_predictions, references=true_labels)
        if data_args.return_entity_level_metrics:
            # Unpack nested dictionaries
            final_results = {}
            for key, value in results.items():
                if isinstance(value, dict):
                    for n, v in value.items():
                        final_results[f"{key}_{n}"] = v
                else:
                    final_results[key] = value
            return final_results
        else:
            return {
                "precision": results["overall_precision"],
                "recall": results["overall_recall"],
                "f1": results["overall_f1"],
                "accuracy": results["overall_accuracy"],
            }

    # data_collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=8 if training_args.fp16 else None)

    # training_args.log_level = 1
    # training_args.save_steps = 2000
    # # To unload the predictions at each 100 prediction.
    # # Otherwise, all the predictions are keep in the GPU and we get a OOM
    # training_args.eval_accumulation_steps = 200

    file_path_train = os.path.join(data_args.train_dir, "merge_train.json")
    data_dict_file_path = {"train": file_path_train}
    file_path_valid = os.path.join(data_args.train_dir, "merge_valid.json")
    data_dict_file_path.update({"valid": file_path_valid})

    datasets = load_dataset("json", data_files=data_dict_file_path, cache_dir=model_args.cache_dir)

    train_dataset = datasets["train"].map(
        tokenize_and_align_labels,
        batched=True,
        num_proc=None
    )

    # ici je vais vouloir loader le model dans poutyne
    # comment pr√©parer le data?

    # val_dataset = datasets["valid"].map(
    #     tokenize_and_align_labels,
    #     batched=True,
    #     num_proc=None,
    #     load_from_cache_file=not data_args.overwrite_cache
    # )

    # trainer = Trainer(
    #     model=model,
    #     args=training_args,
    #     train_dataset=train_dataset,
    #     eval_dataset=val_dataset,
    #     tokenizer=tokenizer,
    #     # data_collator=data_collator,
    #     compute_metrics=compute_metrics
    # )

    # # Train phase
    # train_result = trainer.train(resume_from_checkpoint=None)
    # metrics = train_result.metrics
    # trainer.save_model()
    #
    # metrics["train_samples"] = len(train_dataset)
    #
    # trainer.log_metrics("train", metrics)
    # trainer.save_metrics("train", metrics)
    # trainer.save_state()
    #
    # # Validation phase
    # metrics = trainer.evaluate()
    # metrics["val_samples"] = len(val_dataset)
    #
    # trainer.log_metrics("val", metrics)
    # trainer.save_metrics("val", metrics)
    #
    # # Test phase per country
    # file_names = os.listdir(data_args.test_dir)
    # files_path = [os.path.join(data_args.test_dir, file_name) for file_name in file_names]
    #
    # # loop on datasets
    # for file_path in files_path:
    #     file_name = file_path.split('/')[-1].split('.')[0]
    #     country = pycountry.countries.get(alpha_2=file_name.upper()).name
    #     print(f"**** test on {country} ****")
    #     data_dict_file_path = {"test": file_path}
    #     datasets = load_dataset("json", data_files=data_dict_file_path, cache_dir=model_args.cache_dir)
    #
    #     test_dataset = datasets["test"]
    #     test_dataset = test_dataset.map(
    #         tokenize_and_align_labels,
    #         batched=True,
    #         num_proc=None,
    #         load_from_cache_file=not data_args.overwrite_cache
    #     )
    #
    #     predictions, labels, metrics = trainer.predict(test_dataset, metric_key_prefix="predict")
    #     predictions = np.argmax(predictions, axis=2)
    #
    #     # Remove ignored index (special tokens)
    #     true_predictions = [
    #         [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
    #         for prediction, label in zip(predictions, labels)
    #     ]
    #
    #     trainer.log_metrics(f"test_{country}", metrics)
    #     trainer.save_metrics(f"test_{country}", metrics)
    #
    #     # Save predictions
    #     output_predictions_file = os.path.join(training_args.output_dir, f"{file_name}_predictions.txt")
    #     if trainer.is_world_process_zero():
    #         with open(output_predictions_file, "w") as writer:
    #             for prediction in true_predictions:
    #                 writer.write(" ".join(prediction) + "\n")


def _mp_fn(index):
    # For xla_spawn (TPUs)
    main()


if __name__ == "__main__":
    config_file = "config.ini"
    # config = file_io.read_config(config_file)
    # file_io.set_env_vars(config)
    main()
